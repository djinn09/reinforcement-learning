# Introduction to Reinforcement Learning (RL)
RL is a subfield of Artificial Intelligence (AI) that allows an agent to learn how to achieve a desired outcome 
or maximize rewards over time, through trial-and-error learning. In this post, I give an introduction to the basics of RL, 
including some common terminology and example problems. I also discuss some of the advantages and drawbacks of RL compared to 
other popular machine learning algorithms. Stay tuned for future posts in which I cover more detail on specific modelling
components of RL and how to best get started with RL.
# What is RL and Why Should You Care About it?
RL is a subfield of AI that deals with how agents should select controls in an environment so as to maximize some notion of cumulative reward. 
On the other hand, if the problem is more naturally expressed in terms of cost minimization, then an agent would minimize some notion of cumulative cost. 
RL is considered by many to be the third generation of machine learning algorithms, after supervised learning (SL) and unsupervised learning (UL).
There are some noticeable differences between RL and SL. First, in RL there is no training data per se (labelled data); instead, the agent must learn from experience in an environment by trial and error (hence the “reinforcement” part of the name) from the immediate reward it receives from being in some state and making some control choice. Second, most RL problems are formalized as Markov decision processes (MDPs), which are a specific type of mathematical optimization problem. In an MDP, assuming cost minimization, an agent must choose controls in such a way as to minimize a cost function known as the cost-to-go.
In SL, every input to a neural network or some other parameterized model has an associated training label so that a tuple or pair is formed for each learning example. Often, these input-output label pairs are stacked on top of each other to form large arrays or "batches" and the learning during each iteration of training is over a "mini-batch", a subset of the overall available training data. I do not go into UL in this article, nor anymore into SL, but note that UL is different from both RL and SL in that it is often looking to find patterns in datasets or associations among the data. UL algorithms are similar to RL algorithms in the sense that neither require labels during training, but RL is about optimal decision making as you continue to learn and collecting the most reward as possible for a single decision maker.
# What is Required to Model RL Problems Effectively?
RL is a modelling technique used to understand and optimize how an agent should behave in a given environment. The key components of RL are the following:
A state space, which is the set of all possible states that the agent can be in;
A control space, which is the set of all possible controls that the agent can take;
A reward function, which specifies the rewards associated with various state-control pairs; and
A transition function, which defines the probabilities of transitioning from one state to another given a control.
RL is a modeling technique that can be used to understand and optimize complex systems. The key components of RL are modeling, value estimation, and 
policy optimization. RL can be used to optimize a system by modeling the system’s environment and reward structure, 
estimating the values of states and controls, and then optimizing the policy to maximize the expected reward.
The modeling component of RL involves understanding the system’s environment and modeling the system’s dynamics. 
The value estimation component of RL involves estimating the values of states and controls in order to identify the optimal policy. 
The policy optimization component of RL involves finding the policy that maximizes the expected reward. 
RL can be used to optimize a wide variety of systems, including but not limited to control systems, robotics, resource allocation systems, game playing agents, and many more possibilities as yet untapped.
RL algorithms work by modeling the interaction between an agent and its environment. The most common formalism is the MDP, but there are other formulations as well depending on the kind and quality of information an agent has for making its decisions. 
MDPs are suitable when the state is completely observable by the agent, and the MDP model accounts for noise or randomness in the environment. Within a MDP, there are RL methods that allow an agent to identify optimal policies over time from trial and error experience— that is the agent learns a set of behavior rules that enable it to maximize its expected cumulative reward. In other words, RL algorithms enable agents to learn how to behave in order to achieve their objectives.
# What are Some of The Potential Applications for RL Technology?
RL technology is a type of AI that is designed to learn from experience and take controls in order to maximize a reward function or minimize a cost function. 
This technology has a wide range of potential applications, including games, finance, and robotics. 
Games provide an ideal environment for RL technology to learn and improve its performance, as there are a fixed set of rules and a clear goal (to win the game). 
Financial applications of RL include automated trading, where the goal is to maximize profits while minimizing risks. In robotics, RL can be used to teach robots how to perform tasks such as navigation, object manipulation, or achieving a desired pose. 
With its ability to learn from experience and take controls to achieve a goal, RL technology has the potential to revolutionize a wide range of industries.




What types of rewards are there in RL?
In reinforcement learning, rewards are a key part of the learning process. Rewards signal to an agent what controls it has taken that are valuable, indicating which ones should be repeated when the same state is visited in the future. There are three main types of rewards: extrinsic rewards, intrinsic rewards, and shaped rewards. Extrinsic rewards are externally provided rewards such as money, points, or something else that is valuable. Intrinsic rewards are generated by the agents themselves based on their internal state and their own model of the environment. Shaped rewards are a combination of extrinsic and intrinsic rewards that have been designed to encourage specific types of behavior. These distinctions come from the field of psychology but have a role in RL problems too.
Rewards play a critical role in reinforcement learning by providing motivation for agents to learn and explore. Without rewards, agents would have no way of knowing which controls are important and would be unlikely to make any progress in completing its assigned task or solve a certain control problem. Rewards provide feedback to the agent, allowing it to learn from its controls and adapt its behavior over time. Furthermore, as is discussed in this article, rewards can be frequent or sparse, and deterministic or stochastic.
Frequent vs. Sparse Rewards in RL:
Frequent rewards are any nonzero rewards an agent receives during every stage of the decision problem; this could be for a finite horizon, or an infinite horizon problem setup. On the other hand, sparse rewards are infrequent as the name implies. Sparse rewards could be given only after many steps, say when an agent wins a game, or completes a desired task.
Deterministic vs. Stochastic rewards in RL:
There are two main types of rewards in reinforcement learning: deterministic rewards and stochastic rewards. Deterministic rewards are those rewards that always occur when an agent reaches a certain state and a certain control is taken. Mathematically, if the agent is in state x and takes control u, it will always receive the same reward r. In the future, I will present more detailed mathematical descriptions of states, controls, and rewards, but for now I explain the RL material at a beginner’s level.
For example, if we have a running robot that runs laps around a track and is rewarded the same amount of points for each lap it completes and nothing in between, then the reward is deterministic and here we can say that the reward is sparse too. On the other hand, stochastic rewards are those that may or may not occur when a certain state is reached or a certain control is taken. For example, if the running robot has a 50% chance of receiving a point every time it completes a lap around the track, then the reward function is stochastic. Stochastic rewards are a more general formulation than deterministic rewards; in games with stochastic rewards, generally more exploration is required of the agent, and it takes longer for an agent to learn the optimal policy.
Costs vs. Rewards in RL problems:
In some books, papers, or tutorials you may notice some authors use costs instead of rewards for their agents. This simply is a matter of optimization. If those authors frame their problem so that they are trying to minimize costs incurred rather than maximize rewards obtained, you will see costs instead of rewards in their works, so replace r with c as the optimization variable in your problem. Additionally, the max operator would be replaced everywhere by the min operator in the problem and algorithm descriptions.
Different Reward Function Examples:
In reinforcement learning, the agent is rewarded for taking controls that lead to successful states. The rewards can be immediate, such as receiving a point for each step taken in the right direction, or they can be delayed, such as receiving a point at the end of the episode if the goal was reached. There are many different ways to define rewards, and the choice of reward function can have a substantial impact on the learning process. For example, rewards that are too small may not provide enough motivation for the agent to learn, while rewards that are too large may result in the agent taking too many risks. Finding the right balance of rewards is an important part of designing a successful reinforcement learning algorithm.
A rewards function is used to define what constitutes a successful or unsuccessful outcome for an agent. Different rewards functions can be used depending on the goal of the agent. For example, if an agent is trying to maximize its score in a game, then a rewards function could give positive rewards for winning and negative rewards for losing. On the other hand, if an agent is trying to reach a target location as quickly as possible, the rewards function could be the negative of the number of steps taken so far to encourage the agent to reach the goal in minimum time. Similarly, the reward function could take into account distance from the goal to where the agent currently is too as an inverse function. An example reward function using distance could be one where the reward decreases as 1/(1+d) where d defines the distance from where the agent currently is relative to a goal location.
Conclusion:
There is no one absolutely correct rewards function for any given task, and different reward functions can lead to different behavior from an agent. It is important to carefully select a rewards function that will encourage the desired behavior from an agent. Often times a specific criterion is defined for optimization and engineers look at the trained policy after the fact to discover what the agent has learned. In this case, the engineers often learn clever strategies for control from analyzing the trained agent’s policy that the engineers never could have thought of before the training, or the new policy could be something that breaks with convention in some field.
Though rewards can be used to shape behavior, there are several limitations to using rewards in a real learning system. First, rewards need to be given frequently in order for the behavior to be maintained. This can be impractical, especially if the reward is something tangible like food, toys, or money. Second, rewards can inadvertently teach agents that they only need to perform a desired behavior when they are being rewarded. Third, rewards can create competition and conflict among agents rather than cooperation in a multi-agent setting.
For these reasons, it is important to consider the limitations of rewards when using them to shape agent behavior in a “real” setting. An alternative is to build a simulation of the real environment, and train the agent in a simulator. Finally, for any realistic agent implementation, I believe in a human in the loop to monitor or have some program checking the agent’s behavior over time after training to verify it is operating in a safe manner, while meeting its goals or desired objectives.





# Different Types of Policies in Reinforcement Learning
Consider a finite set of states and controls. There are two main types of policies that may be defined for reinforcement learning problems. The two main types of policies in reinforcement learning are 1) deterministic policies and 2) stochastic policies.
1) Deterministic Policies:
In a deterministic policy, the same control is always output for the same input state. Usually the phi symbol denotes a policy function, which takes as input a state from the state space and always output the same control from the control space. In general, the goal of RL is to learn the optimal controls for all possible states so that sum of all agent rewards is maximized.
Sometimes, a deterministic policy doesn't cut it for designers or engineers either due to competition among agents, partially observed system state, or other sources of noise that warrant stochastic policies. In such challenging circumstances, a stochastic policy is more appropriate.
2) Stochastic Policies:
On the other hand, a stochastic policy is only a little bit more complicated. A stochastic policy says that for any given state, each control is selected with some possibly non-zero probability. One can think that for stochastic policies the agent conditions on the encountered state and selects controls according to a probability distribution. Because a stochastic policy is a distribution it must obey some rules.
Since we are dealing with the realm of finite spaces in this article the distribution is a discrete probability mass function. The first rule is a sum rule, which says that for each possible state, the sum over all possible controls, the stochastic policy must sum to 1. These are the main differences between deterministic and stochastic policies.

# What is State in Reinforcement Learning?
Simple answer: the "state" is whatever the engineer says it is!
A state in reinforcement learning is a representation of the current environment that the agent is in. This state can be observed by the agent (and is most often deterministic or fully observed), and it includes all relevant information about the environment that the agent needs to know in order to make a decision. The state can be represented as a vector of real-valued numbers, or could even consist of Boolean variables. In addition, agents can store states in its memory, and recall them at any time during the learning/training process. This sort of memory is not required for Markov systems where the agent only needs to know the current state to act optimally. However, in non-Markov systems memory is more important to act optimally.
In reinforcement learning, state is defined as the set of information that an agent has about the environment at a given time. This information can include the agent’s current location, the objects around it, and any previous states that have been visited, possibly with controls taken and any rewards received. If more than current state is included in the current decision making process, that is also referred to as history or experience. The state is important because it allows the agent to plan its next controls based on the current situation and its goals. Without state, an agent would have no way of knowing what it has done in the past or what it needs to do in the future.
Related Question: What is State Space in Reinforcement Learning?
In reinforcement learning, the state space is the set of all possible states that an agent can be in. This includes both the current state and all future states that could be reached from the current state. The state space also defines the set of all possible controls that an agent can take and the set of all possible rewards that an agent can receive. It is important to note that the state space is not necessarily finite; it can also be infinite depending upon the model used. For example, if an agent is learning to play a game, the state space would consist of all of the possible game states that could be reached from the current state. If the game has an infinite number of possible states, then the state space would also be infinite.
For a specific game example, in a game of chess, the state space would include all of the possible configurations of pieces on the board. In order to choose the best possible control at each step, the agent needs to be able to keep track of all of the possible states and identify which state it is currently in. The state space can be very large and complex, particularly in games with a large number of possible states (although still finite). However, reinforcement learning algorithms are designed to efficiently explore and learn from state spaces in order to find optimal policies.
Examples of Some Reinforcement Learning Problems:
There are many other examples of reinforcement learning problems outside of board games as well. For instance, in business, companies often use reinforcement learning algorithms in order to optimize their advertising campaigns or stock portfolios. In medicine, reinforcement learning has been used to develop models that can predict how well a patient will respond to different treatments. And in robotics, reinforcement learning is often used to teach robots how to complete tasks such as moving objects from one place to another.
Reinforcement learning has been successfully applied to a wide variety of examples, ranging from gaming applications to industrial control. Some more examples of states in reinforcement learning problems include: 1) robots moving through an environment, 2) automated collection of data, 3) automated stock trading, 4) energy management in buildings, and 5) anticipation of human driver error in self-driving vehicles. In each of these examples, the agent is trying to learn how to optimally interact with its environment in order to achieve a goal. This can be a difficult task, as the agent often does not have complete information about the environment or the long-term consequences of its controls. However, by carefully modeling the problem and using trial and error, the agent can learn how to best achieve its goals.
Final Thoughts on State in Reinforcement Learning:
In reinforcement learning, the agent is often faced with a complex task and must learn how to complete it through trial and error. A simple or reduced conception of the task or world in terms of a compressed state can potentially make solving the problem easier. This can be an effective way to train an agent, but it can also lead to some issues such as loss of key information relevant to making optimal decisions. Additionally, the agent may not be able to learn from its mistakes if the task is too complex. Finally, reinforcement learning can be very time-consuming and may not always converge on a solution. Despite these potential drawbacks, reinforcement learning remains a popular approach in artificial intelligence research.
Other issues with state in reinforcement learning are, how do we best deal with many questions such as how to use past states to determine the present or future state of the system. What are the optimal strategies for exploration and exploitation? How can we ensure that the agent does not become trapped in a local optimum? A proper state formulation can help with some of these complex issues but state also comes with its own set of problems. Also, tracking state can be difficult, especially in large or highly complex systems. In addition, using state can be computationally expensive, which can limit the number of trials or episodes an agent can run in a given period of time. As a result, reinforcement learning algorithms that use state often need to trade off between accuracy and computational efficiency. This is usually referred to as “the curse of dimensionality”.
In general, state can be complex and difficult to work with in reinforcement learning. While it can provide helpful information about the environment, it can also lead to issues such as partial observability. Reinforcement learning problems can be broadly classified into two types: fully observable and partially observable (although there are other possible characterizations). With a fully observable state, the agent has complete information about the environment. This means that the agent knows the state of all relevant variables at each stage of decision making. However, in a partially observable state, the agent only has access to a limited set of variables or none through direct observation. As a result, the agent must use its observations to infer the underlying state of the environment. Many real-world problems are only partially observable, making them challenging for reinforcement learning agents. Some examples of these problems include robotics, natural language processing, and financial modeling. In each of these domains, the agent must learn to extract useful information from partial observations in order to make good control decisions.

# What Kinds of Controls are Possible in Reinforcement Learning Problems?

What is a reinforcement learning problem and why are controls important for them?
Reinforcement learning is a problem where an agent tries to learn how to maximize both its short-term and long-term reward by interacting with its environment. Decisions, also known as controls, are important for reinforcement learning problems because they help agents learn which control in a given state is associated with the greatest rewards. There are many different reinforcement learning algorithms that help discover what are the best controls (those controls that lead to the greatest rewards); future tutorials, will cover these algorithms in greater depth.
Different reinforcement learning control algorithms can be combined to overcome some of their individual weaknesses. For example, reinforcement learning algorithms could encourage exploration early on during an agent’s training in order to avoid getting stuck in local optima or suboptimal policies. In addition, reinforcement learning control algorithms can be used to help an agent recover from mistakes and return to the task at hand. Properly tuning reinforcement learning control selection is an important part of solving reinforcement learning problems. After an agent has been trained sufficiently, it would then switch to exploiting its knowledge to obtain the greatest possible sum of rewards from any given initial state during the implementation or testing phase. For more on states read:
https://www.linkedin.com/pulse/what-state-reinforcement-learning-caleb-m-bowyer/?trackingId=bYeK2sYiSmuV3Sdktpyglg%3D%3D
The set of all possible controls is referred to as the control space. This space, in most modelled problems, is finite, especially for games. However, the control space can easily be infinite in problems where continuous controls are required. Think for a moment on some controls: pressure or temperature (all of which are continuous controls).
In reinforcement learning, the state-control space is often too large for the agent to search exhaustively, so it must use some form of approximation in order to focus its search. There are two main types of reinforcement learning algorithms: value-based and policy-based. Value-based methods aim to directly estimate the value function, which gives the expected return for taking a given control in a given state. Policy-based methods aim to directly learn a policy, which is a mapping from states to controls.
Also, policies can be either deterministic or stochastic. Both types of reinforcement learning algorithms have their advantages and disadvantages, and there is significant ongoing research into developing new and improved reinforcement learning control algorithms.
Some reinforcement learning control algorithms:
Reinforcement learning algorithms can be used to solve a variety of different types of control problems. Some of the most common reinforcement learning control algorithms include:
Q-learning: this algorithm is used to find the optimal policy for an agent in a Markov Decision Process (MDP). Q-learning can be a model-free or a model-based reinforcement learning algorithm, meaning that it does not use a model of the environment in its Q-function update or it does.
SARSA: the SARSA algorithm is similar to Q-learning, but it uses a slightly different update rule. SARSA is also a model-free reinforcement learning algorithm.
Monte Carlo methods: Monte Carlo methods are reinforcement learning algorithms that can be used to solve both MDPs and Partially Observable MDPs (POMDPs). Monte Carlo methods are typically either on-policy or off-policy. On-policy reinforcement learning algorithms learn from experience sampled from the current policy, while off-policy reinforcement learning algorithms can learn from experience sampled from any arbitrary policy, even if that policy is not the optimal policy.
Deep Q-Networks (DQNs): these are non-linear function approximators and learn very powerful policies. DQNs are capable of learning complex relationships between input states and output Q-values. The future of RL seems to be heading in the DQN direction.
Each of these reinforcement learning algorithms has its own strengths and weaknesses, so it is important to choose the right algorithm for the specific problem that you are trying to solve.
How to choose the right type of control algorithm for your RL problem?
There are many different types of reinforcement learning control algorithms, and the choice of which one to use depends on the specific optimization problem that needs to be solved. For example, if the goal is to find the optimal path through a maze, then an algorithm such as tabular Q-learning would be well suited as the problem can be broken down into a finite set of states and a finite set of controls. The states are the maze positions, and the controls could be move up, down, left, or right.
However, if the goal is to control a robotic arm to move objects around in a workspace, then a different reinforcement learning control algorithm, such as an inverse reinforcement learning algorithm, would be more appropriate. The choice of reinforcement learning control algorithm is an important one, and it is important to select the right algorithm for the specific optimization problem that needs to be solved.
Some reinforcement learning problems are more suited to tabular methods, while others are more suited to function approximation methods such as using neural networks for control - DQNs. The second thing to consider when selecting a reinforcement learning control algorithm for your problem is the size of the state space and control space. Tabular methods are only feasible for small state spaces, while function approximation methods can be used for larger state spaces. The third thing to consider is the nature of the reward function. Some reinforcement learning problems have sparse rewards, while others have dense rewards. An example of sparse rewards, is when an agent is only rewarded for winning a game that carries on for many stages of decision making, and for every other stage receives a zero reward signal.
Finally, you should consider the computational resources available to you. Some reinforcement learning algorithms are more computationally intensive than others. If you have limited resources, you may need to use a less computationally intensive algorithm.
Examples of how different controls can be used in real-world reinforcement learning problems:
Reinforcement learning controls can be used in a variety of real-world problems. For example, consider a robotic arm that needs to be trained to reach for a specific object. In this case, reinforcement learning can be used to tune the arm’s control parameters so that it reaches the target with the highest probability. In robotic controls, reinforcement learning is used to optimize the control inputs of a robot so that it achieves some goal. For example, a robot might be programmed to wander around a room and collect all the objects in the room. The reinforcement learning algorithm would learn the best way to control the robot’s motors so that it can collect all the objects as quickly as possible.
In game playing, reinforcement learning is used to train an artificial intelligence (AI) to play a game such as chess or Go. Here, the control would be where to place what game piece, adhering to the rules of the game. The AI agent is programmed with a reinforcement learning algorithm, which allows it to learn from its mistakes and gradually get better at playing the game. Eventually, the AI agent will be able to beat human opponents at the game.
Another example of reinforcement learning controls in action is autonomous driving. Here, reinforcement learning can be used to optimize the control system so that the car drives safely and efficiently. Additionally for autonomous driving, the car could also learn the best way to navigate through traffic, based on information such as the current traffic conditions and the destination.
In an agricultural setting, reinforcement learning could be used to control irrigation systems. The system would learn the optimal amount of water to provide to the plants, based on factors such as weather conditions and the type of plant. Other applications of reinforcement learning in a gardening setting, could be where to place seeds and at what depth to plant those seeds.
In a factory setting, reinforcement learning could be used to control assembly line robots. The system would learn the most efficient way to assemble products, based on data such as the number of products to be assembled and the order in which they need to be assembled. Reinforcement learning is a powerful tool that can be used in a variety of different settings, including industrial settings.
In the medical space, reinforcement learning can also be used to control robotic prostheses. In this application, reinforcement learning can be used to adjust the control parameters of the prosthesis so that it functions optimally for the user. Thus, reinforcement learning controls can have a wide range of real-world applications. I am most excited about this for future research as it seems to be the least explored, but one of the most useful for helping humanity in myriad ways.
In conclusion, reinforcement learning controls can be used in a variety of ways to optimize performance or solve specific control problems for very challenging problems where the best strategy for control is probably not known currently. For instance, they can be used to calculate the optimal control inputs for any system given a set of constraints and objectives, or to find the best way to control a possibly nonlinear system. As such, reinforcement learning control algorithms represent a powerful tool that can be used in many different ways to improve the performance of highly complex systems that are nonlinear, noisy, or continuous.
